<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="author" content="Luna Tian"/><meta name="keyword"/><meta name="description" content="Luna Tian Today, the question we face may not be â€œWhat can we still trust?â€ but rather, â€œHow can we live responsibly in an age where truth itself is unstable?â€   AI, like every other transformative te">
<meta property="og:type" content="article">
<meta property="og:title" content="Can Media Literacy Save Us from the Illusions of AI?">
<meta property="og:url" content="https://lunasblog.com/2025/10/22/media-literacy/index.html">
<meta property="og:site_name" content="Luna Tian">
<meta property="og:description" content="Luna Tian Today, the question we face may not be â€œWhat can we still trust?â€ but rather, â€œHow can we live responsibly in an age where truth itself is unstable?â€   AI, like every other transformative te">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://lunasblog.com/img/pope.png">
<meta property="article:published_time" content="2025-10-22T18:17:07.000Z">
<meta property="article:modified_time" content="2025-10-22T18:22:08.099Z">
<meta property="article:author" content="Luna Tian">
<meta property="article:tag" content="AI and Journalism">
<meta property="article:tag" content="Media Literacy">
<meta property="article:tag" content="Education">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lunasblog.com/img/pope.png"><title>Can Media Literacy Save Us from the Illusions of AI? - Luna Tian - Journalist & Blogger</title><link rel="shortcut icon" href="/img/site-icon.png">
<link rel="stylesheet" href="/css/style.css" id="dm-light">


<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css">
<meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="top-nav" ondblclick="scrollToTop()"><div class="nav-info"><div class="nav-icon"><img id="nav-icon" src="/img/site-icon.png"/></div><div class="nav-title"><a id="nav-title" href="/" title="Home">Luna Tian</a></div></div><div class="nav-ribbon"><div class="top-menu-expanded"><a class="top-menu-item" href="/archives"><span>Archives</span></a><a class="top-menu-item" href="/categories"><span>Categories</span></a><a class="top-menu-item" href="/tags"><span>Tags</span></a><a class="top-menu-item" href="/about"><span>About</span></a></div><div id="top-menu-btn" onclick="openTopMenu()" title="Open Menu"><i class="fa-solid fa-bars fa-lg"></i></div></div></div></header><div id="top-menu-hidden"><div class="menu-hidden-content"><div class="menu-hidden-nav"><a class="menu-hidden-item" href="/archives"><i class="fa-solid fa-box-archive fa-sm"></i><span>Archives</span></a><a class="menu-hidden-item" href="/categories"><i class="fa-regular fa-folder-open fa-sm"></i><span>Categories</span></a><a class="menu-hidden-item" href="/tags"><i class="fa-solid fa-tags fa-sm"></i><span>Tags</span></a><a class="menu-hidden-item" href="/about"><i class="fa-solid fa-paw fa-sm"></i><span>About</span></a></div></div><div class="menu-hidden-blank" onclick="closeTopMenu()"></div></div>
<div class="blog-info"><div class="blog-pic"><img id="blog-pic" src="/img/site-icon.png"/></div><div class="blog-title"><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i><span>Luna Tian</span><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i></div><div class="blog-desc">Tracking freedom, truth, and memory â€” one story at a time.</div></div><div class="main"><div class="main-content"><article class="post"><div class="post-title"><h1><i class="fa-solid fa-paw"></i>Can Media Literacy Save Us from the Illusions of AI?</h1></div><div class="post-info"><div class="post-info-first-line"><div class="post-date"><i class="icon fa-regular fa-calendar-plus" title="Published date"></i><time class="publish-time">2025-10-22</time><!-- i.icon.fa-regular.fa-calendar-check(title= __('datetime.updated'))--><!-- time.update-time= date(post.updated)--></div>
<div class="post-categories"><i class="icon fa-regular fa-folder-open" title="Categories"></i><a class="post-category" href="/categories/Research/">Research</a></div>
<div class="post-tags"><i class="icon fa-solid fa-tags" title="Tags"></i><a class="post-tag" href="/tags/AI-and-Journalism/">AI and Journalism</a><i class="icon fa-solid fa-tags" title="Tags"></i><a class="post-tag" href="/tags/Media-Literacy/">Media Literacy</a><i class="icon fa-solid fa-tags" title="Tags"></i><a class="post-tag" href="/tags/Education/">Education</a></div></div><div class="post-info-second-line"><div class="post-copyright"><i class="icon fa-brands fa-creative-commons" title="Copyright"></i><span>Copyright: </span><a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" title="CC BY-NC-ND 4.0">BY-NC-ND 4.0</a></div>
<div class="post-word-count"><i class="icon fa-solid fa-pen-to-square"></i><span>About 3357 words</span></div></div></div><div class="post-content"><p>Luna Tian</p>
<p>Today, the question we face may not be <em>â€œWhat can we still trust?â€</em> but rather, <em>â€œHow can we live responsibly in an age where truth itself is unstable?â€</em>  </p>
<p>AI, like every other transformative technology, has no reverse gear. Once it starts, it doesnâ€™t stop. The line between the real and the artificial will only continue to blur as AI-generated content multiplies.  </p>
<p>Perhaps media literacy programs, which we keep reading about, embody a belief â€” that education can help people reclaim control over truth.<br>But maybe the point is not about <strong>control</strong>, but about <strong>coexistence</strong>.</p>
<hr>
<h2 id="I-The-Pope-in-a-Puffer-Jacket"><a href="#I-The-Pope-in-a-Puffer-Jacket" class="headerlink" title="I. The Pope in a Puffer Jacket"></a>I. The Pope in a Puffer Jacket</h2><p>I still remember one of the most striking pieces of news from March 2023:<br>Pope Francis appeared online wearing a white Balenciaga puffer coat.  </p>
<p align="center">
  <img src="/img/pope.png" width="80%">
  <br>
  <em>Fake photos of Pope Francis in a puffer jacket.</em>
</p>

<p>The photo made me laugh. There he was, calm and dignified, walking down the street in that voluminous, glossy jacket â€” a sharp contrast to his usual modest robes. The coat was beautiful, its fabric full and rich in texture.  </p>
<p>Within hours, the image went viral, shared millions of times. Some people laughed like I did; some discussed the Catholic Churchâ€™s public image strategy; others admired the outfit, even asking me whether Iâ€™d consider buying one.<br>For a moment, I honestly thought it was part of a marketing campaign.  </p>
<p>Later, I learned that the image was created by a construction worker in Chicago. He had simply typed a few words into Midjourney â€” â€œPope,â€ â€œBalenciaga,â€ â€œstreet photo.â€ Seconds later, the algorithm produced it: the lighting perfect, the texture of the coat realistic, even the Popeâ€™s slightly hunched posture eerily convincing.</p>
<p>Three days later, fact-checking organizations confirmed that it was AI-generated.<br>But by then, the image had already been reprinted by thousands of media outlets, discussed endlessly online. In some sense, it had already <em>become an event.</em></p>
<p>I noticed one user on Twitter who asked:  </p>
<blockquote>
<p>â€œIf even this is fake, what can we still believe in?â€</p>
</blockquote>
<p>At the time, I didnâ€™t have an answer.<br>Eight months later, UNESCO published the <em>Guidelines for the Governance of Digital Platforms</em> â€” a document that tried to impose order on our chaotic digital landscape.  </p>
<p>Its core recommendations could be summarized as three principles:  </p>
<ol>
<li>Foster critical thinking  </li>
<li>Promote fact-checking  </li>
<li>Urge platforms to label AI-generated content</li>
</ol>
<p>Learn to distinguish the true from the false â€” and you can remain clear-minded amid AIâ€™s illusions. That was its logic.</p>
<p>The first time I read this document was in May last year, while finishing a term paper. It was nearly summer; everything felt heavy; I was in a bad mood â€” and, as luck would have it, it was raining.  </p>
<p>Still, I found the document well written. Iâ€™ve always had a soft spot for texts filled with goodwill and idealism.<br>But I stopped at page nineteen.  </p>
<p>There, it said: platforms should be <em>urged</em> to label AI-generated content. Not <em>required</em>, not <em>mandated</em>, but <em>urged</em> â€” a word so polite itâ€™s nearly transparent.  </p>
<p>That night, out of boredom, I opened ChatGPT and asked it to generate a short paragraph about media literacy, citing three academic papers.  </p>
<p>It responded instantly: eloquent prose, coherent structure, three papers listed with author names, titles, and journal names.  </p>
<p>When I checked the references, none of them existed.</p>
<p>It wasnâ€™t the first time, and yet I couldnâ€™t help laughing. The authorsâ€™ names were real; the journals were real; the paper titles sounded perfectly plausible â€” but the works themselves had never been written.  </p>
<p>I asked the AI to describe these â€œpapers.â€<br>Its made-up arguments sometimes sounded more persuasive than real scholarship â€” as if they existed somewhere in a parallel universe, waiting to be cited. The thought sent a shiver through me.</p>
<p>So, is our biggest challenge still <em>â€œhow to tell true from falseâ€</em>?<br>I think not. The real question is: <strong>what counts as truth?</strong></p>
<hr>
<h2 id="II-The-Illusion-of-Stability"><a href="#II-The-Illusion-of-Stability" class="headerlink" title="II. The Illusion of Stability"></a>II. The Illusion of Stability</h2><p>If we look back â€” and â€œbackâ€ may mean only two years ago now â€” UNESCOâ€™s guidelines were born in a peculiar moment of history.  </p>
<p>Letâ€™s return to 2023: ChatGPT had just entered the public eye; Midjourney images were flooding social media; governments worldwide were scrambling to respond.<br>UNESCOâ€™s chosen path was the familiar one: education.</p>
<blockquote>
<p>â€œEmpowering users â€” including through media and information literacy education â€” is fundamental to countering disinformation and promoting healthy online participation,â€<br>the document reads on page eleven.</p>
</blockquote>
<p>Itâ€™s a familiar logic.<br>It assumes the problem lies with the user â€” not as a form of blame, but as a premise: if people can be made smarter and more critical, then they can resist misinformation.  </p>
<p>Learn critical thinking, practice fact-checking, and youâ€™ll survive the data flood.  </p>
<p>But the language of the document is telling. Words like â€œshould,â€ â€œencourage,â€ and â€œpromoteâ€ abound, while â€œmustâ€ and â€œrequireâ€ are scarce.  </p>
<p>When it comes to platform responsibility, the tone becomes almost evasive.<br>In discussing algorithmic transparency, the text suggests that platforms <em>â€œshould considerâ€</em> disclosing information about their recommendation systems.</p>
<p><em>Should consider</em> â€” meaning, of course, they could also <em>not</em> consider it.</p>
<p>I closed my laptop and went for a walk, still thinking about what the document <em>didnâ€™t</em> say.<br>What happens when there are no penalties, no oversight bodies, no accountability?<br>What if platforms simply choose not to label AI content?<br>If algorithms remain opaque, who is responsible?</p>
<p>Unfortunately, the document is silent on that.</p>
<p>I recalled a Zoom conference I once attended. A policymaker said on stage:  </p>
<blockquote>
<p>â€œWe must empower every citizen to distinguish truth from falsehood.â€  </p>
</blockquote>
<p>Thunderous applause followed.  </p>
<p>During the Q&amp;A, a woman introduced herself as a high school teacher and asked:  </p>
<blockquote>
<p>â€œIf my students barely have time to finish their homework, how much time should they spend verifying the news every day?â€  </p>
</blockquote>
<p>Moments later, I logged out.<br>The panelists praised her â€œexcellent question,â€ promised to â€œadd more class hours,â€ and moved on.<br>I stood by the library window. It was already getting dark.  </p>
<p>Letâ€™s imagine, then, a thought experiment.</p>
<p>Suppose you are a responsible citizen.<br>You follow every UNESCO recommendation.</p>
<p>One day, you read a piece of news: a certain government is secretly developing a new weapon.<br>You check the authorâ€™s profile â€” it looks legitimate.<br>You search for corroboration â€” several small media outlets have reported the same.<br>You find documents that look official.<br>You cross-check multiple sources. Everything aligns.  </p>
<p>So, you share it.</p>
<p>Three days later, a fact-checking organization announces that it was a coordinated disinformation campaign.<br>Those â€œsmall media outletsâ€ were fabricated. The â€œofficial documentsâ€ were AI-generated.<br>The â€œordinary userâ€ was a bot.<br>The entire event was an AI-driven, multilayered illusion.</p>
<p>As I once wrote in another essay, <em>â€œThings That Look Like News,â€</em> â€” a text that <em>looks</em> like journalism feels trustworthy. Likewise, something that <em>looks</em> like truth can bypass our verification instincts.</p>
<p>You did everything right. You followed every guideline.<br>And still, you were fooled.</p>
<p>Because the rules of the game have changed.</p>
<hr>
<h2 id="III-After-the-Enlightenment"><a href="#III-After-the-Enlightenment" class="headerlink" title="III. After the Enlightenment"></a>III. After the Enlightenment</h2><p>The French philosopher Antoinette Rouvroy once introduced a concept called <strong>â€œalgorithmic governmentalityâ€</strong> during a talk at the <em>Society of the Query</em> conference.  </p>
<p>She explained it simply:<br>In the digital age, power no longer operates by issuing orders; it operates by shaping environments.<br>Platforms donâ€™t have to tell you what to see â€” they just adjust the algorithms, and youâ€™ll naturally see what they want you to see.</p>
<p>So, does media literacy still matter?</p>
<p>Maybe.<br>But if it only teaches you how to find your way out of a maze without ever asking <em>why the maze exists</em>, it risks becoming another tool of governance.<br>Education should teach us to question and to critique.</p>
<p>What does an ideal human being look like?<br>Perhaps rational, autonomous, capable of independent judgment â€” the Enlightenment model of <em>Descartesâ€™ â€œI think, therefore I amâ€</em> and <em>Kantâ€™s â€œDare to know.â€</em>  </p>
<p>In that tradition, humans are subjects; technology is a tool. The boundary between them is clear.  </p>
<p>But does that boundary still exist?</p>
<hr>
<p>These days, I have new habits that I never used to have.<br>When I hit a wall in writing, I used to email a trusted friend or text a kind professor. Weâ€™d discuss ideas, and Iâ€™d often come away inspired.<br>It slowed my writing, but those conversations were warm and human. I remember the office hours that turned into dinners at favorite restaurants. Iâ€™ve never been a solitary writer; I like the feeling of thinking <em>with</em> others.</p>
<p>ChatGPT has changed that.<br>Although I avoid using AI in my essays or academic work, I still appreciate its instant feedback â€” the kind that comes without judgment or social weight.<br>Sometimes I ask it questions, seek suggestions, explore threads of thought.  </p>
<p>But mid-conversation, I often pause and wonder: <em>Who is writing this?</em><br>Is it me, or the AI?<br>Or have we, somehow, co-authored something?</p>
<p>I recall reading the literary theorist N. Katherine Hayles, who developed the concept of the <strong>â€œposthuman.â€</strong><br>She observed that our thinking, memory, and judgment are now so deeply embedded in technical systems that itâ€™s nearly impossible to say where the human ends and the machine begins.</p>
<p>As someone who grew up reading science fiction, I once found that idea exciting.<br>Now that Iâ€™m living it, Iâ€™m not sure whether to call it a privilege or a tragedy.</p>
<p>Hayles wrote:  </p>
<blockquote>
<p>â€œThe question is not how to resist AIâ€™s influence, but how to remain ethical, reflective, and responsible within a state of coexistence.â€  </p>
</blockquote>
<p>Traditional media literacy teaches us to be better <em>independent judges</em>.<br>But what if judgment itself is no longer an independent act?<br>If AI not only transmits information but shapes how we perceive the world â€” does â€œindependent judgmentâ€ still mean anything?</p>
<p>A few days ago, I called a classmate who studies the philosophy of AI.<br>I told him about my anxiety â€” about how overwhelming this all feels â€” and asked what troubled him most about large language models.  </p>
<p>He thought for a while and said:  </p>
<blockquote>
<p>â€œTheyâ€™re not reproducing human knowledge. Theyâ€™re generating a new kind of knowledge.â€  </p>
</blockquote>
<p>â€œWhat do you mean?â€ I asked.  </p>
<p>â€œTraditionally,â€ he said, â€œknowledge comes from evidence, experience, reasoning. A statement is true because it corresponds to reality.<br>But GPT doesnâ€™t work that way. It calculates statistical relationships between words. It predicts which sentences sound plausible.<br>The result is something neither true nor false â€” just <em>apparently believable</em>.â€  </p>
<p>I told him about the fake citations ChatGPT had produced for me.<br>He said, â€œThatâ€™s not deception in the traditional sense. Itâ€™s a new form of production. Itâ€™s not giving you lies â€” itâ€™s giving you <em>possibilities</em>.â€</p>
<h2 id="IV-What-Do-We-Need"><a href="#IV-What-Do-We-Need" class="headerlink" title="IV. What Do We Need?"></a>IV. What Do We Need?</h2><p>Thereâ€™s no simple answer.<br>But perhaps we can begin by reframing the question.</p>
<p>Not â€œIs this true or false?â€ â€” but  </p>
<blockquote>
<p>â€œHow was this created?â€<br>Not â€œHow can I identify AI content?â€ â€” but<br>â€œHow is AI shaping the way I think?â€<br>Not â€œHow do I protect myself?â€ â€” but<br>â€œHow do we collectively build a more transparent environment?â€</p>
</blockquote>
<p>This requires a new kind of awareness.</p>
<p>First, a reflective mindset.<br>It may be hard to reject AI entirely in daily life, but staying conscious while using it is still possible.<br>When you ask ChatGPT to help write an email, when you accept algorithmic news recommendations, when you feel convinced by a well-written post â€” pause. Step back. Ask yourself:  </p>
<blockquote>
<p>Why does this feel trustworthy?<br>Because it makes sense? Or because it aligns with what I already want to believe?</p>
</blockquote>
<p>Itâ€™s not about one-time fact-checking. Itâ€™s about cultivating a state of ongoing dialogue â€” a kind of <strong>awareness</strong>.  </p>
<p>Even if youâ€™re co-creating meaning with AI, you can still remain conscious of that process.</p>
<p>Then, thereâ€™s systems thinking.<br>Information doesnâ€™t appear out of nowhere. It is produced, filtered, and circulated within complex technological, economic, and political systems.<br>When you read a headline that begins with <em>â€œStudies showâ€¦â€</em>, donâ€™t just ask whether the study exists â€” ask:  </p>
<blockquote>
<p>Who funded this study?<br>Where did the data come from?<br>Why is this narrative being amplified <em>now</em>?</p>
</blockquote>
<p>Some technical literacy is also helpful.<br>You donâ€™t need to learn how to code, but understanding the basics helps:  </p>
<ul>
<li>How do recommendation algorithms determine what we see?  </li>
<li>Why do language models hallucinate?  </li>
<li>How does training data shape AI bias?</li>
</ul>
<p>We donâ€™t need technical mastery.<br>We need <strong>power literacy</strong> â€” to understand <em>who</em> shapes our information environment, and <em>how</em>.</p>
<p>But most important of all: we must shift from <strong>individual responsibility</strong> to <strong>collective action</strong>.</p>
<p>No matter how smart you are as an individual, you canâ€™t fight systemic manipulation alone.<br>You may learn to detect fake news. But if an algorithm is pushing carefully designed disinformation to a billion users every day â€” your personal resistance wonâ€™t be enough.</p>
<p>We need a concept of <strong>digital citizenship</strong>.<br>The right to know what algorithms are doing to you.<br>The right to control how your data is used.<br>The right to demand explanations when AI systems make decisions that affect your life.<br>The right to opt out of personalized recommendations.  </p>
<p>These arenâ€™t gifts from tech companies.<br>They are <strong>rights</strong> â€” to be won through legislation, regulation, and collective negotiation.</p>
<p>The EU is already experimenting.<br>The <strong>AI Act</strong> passed in 2024 mandates that high-risk AI systems must be explainable and human-supervised.<br>The <strong>Digital Services Act</strong> gives users the right to opt out of personalization, and forces major platforms to explain their algorithmic logic to regulators.</p>
<p>These laws are imperfect, their enforcement full of challenges.<br>But they acknowledge one essential truth: <strong>we cannot place all responsibility on the user</strong>.</p>
<p>Some say the EU is too conservative.<br>But I think thereâ€™s value in trying to pull the reins.</p>
<p>Of course, legislation always lags behind.<br>By the time weâ€™re regulating GPT-4, GPT-5 is already here.<br>But thatâ€™s not a reason to give up â€” itâ€™s a reason to act faster, and smarter.</p>
<p>The hardest question to ignore is: <strong>what responsibility do the platforms bear?</strong></p>
<p>Why are we always talking about â€œeducating users,â€<br>but almost never about <strong>regulating platforms</strong>?</p>
<p>If a food company sells toxic food, we donâ€™t say, <em>â€œConsumers should learn to detect poison.â€</em><br>But in the digital world, weâ€™ve accepted a strange logic: platforms can design systems that spread lies â€” then blame the user for believing them.</p>
<p>That logic must be challenged.<br>Algorithms decide what gets seen, amplified, and recommended.<br>If thatâ€™s editorial power, then <strong>editorial responsibility must follow</strong>.</p>
<p>Concretely:  </p>
<ul>
<li>Platforms must make recommendation algorithms transparent to regulators.  </li>
<li>If disinformation causes real-world harm, platforms should be held legally accountable.  </li>
<li>Users should have the right to opt out of personalization.</li>
</ul>
<p>As for <em>â€œAI-generated content must be labeledâ€</em> â€” honestly, that may be the most difficult goal of all.</p>
<p>There are enormous technical challenges.<br>Who decides what counts as â€œAI-generatedâ€?<br>What if a text is 50% AI-written, 50% human-edited â€” does it count?<br>And what about enforcement? Anonymous uploads? Cross-border platforms?</p>
<p>Maybe perfection isnâ€™t the goal.<br>Maybe the goal is <strong>procedural justice</strong>.</p>
<p>In high-risk domains â€” journalism, medicine, legal documents â€” disclosure of AI use should be mandatory.<br>Platforms can offer creators labeling tools.<br>We can invest in detection technologies â€” though it may become an endless arms race.</p>
<p>The point is not to design a perfect system,<br>but to <strong>stop offloading all the burden onto individuals</strong>.</p>
<hr>
<h2 id="V-Living-in-the-Flow"><a href="#V-Living-in-the-Flow" class="headerlink" title="V. Living in the Flow"></a>V. Living in the Flow</h2><p>As I write this, itâ€™s not raining â€” but it is windy.<br>The temperature dropped nearly five degrees today.<br>I might need my down jacket tomorrow.</p>
<p>Speaking of jacketsâ€¦<br>That image of the Pope in the puffer coat comes back to me.<br>It looked so real. I even thought it was brand marketing. But it wasnâ€™t. It was misinformation.  </p>
<p>And when people found out, they didnâ€™t get angry.<br>They got confused.</p>
<blockquote>
<p>â€œIf even this is fake, what can we still believe in?â€</p>
</blockquote>
<p>Maybe that question itself is flawed.</p>
<p>The point isnâ€™t <em>what</em> we can still believe in â€”<br>but <em>how</em> we live <strong>ethically in an age where truth is fluid</strong>.</p>
<p>Like all technology, AI canâ€™t be un-invented.<br>Weâ€™re heading into a future where content will be increasingly generated,<br>and the line between fact and fiction will continue to dissolve.  </p>
<p>We cannot return to a time when truth was clear â€”<br>and perhaps that time never existed.</p>
<p>Even before AI, we were already surrounded by misinformation.</p>
<p>Still, I believe UNESCOâ€™s media literacy framework represents a sincere conviction â€”<br>a belief that education can help us regain control over truth.</p>
<p>But <strong>control</strong> is not the answer.<br><strong>Coexistence</strong> is.</p>
<p>That doesnâ€™t mean abandoning judgment.<br>It means cultivating a <strong>more complex kind of literacy</strong> â€”<br>one that reflects while using AI, questions the logic of production,<br>and seeks <strong>collective change</strong> beyond personal efforts.</p>
<p>It means recognizing that while we can no longer stand <em>outside</em> technology,<br>we can still remain <strong>awake within it</strong>.</p>
<p>Last fall, while giving a presentation, a student sighed and said,  </p>
<blockquote>
<p>â€œIt sounds like we canâ€™t be sure of anything anymore.â€</p>
</blockquote>
<p>I thought for a moment and replied:  </p>
<blockquote>
<p>â€œItâ€™s not that nothing can be certain. Itâ€™s that the <em>way</em> we become certain has changed.<br>We used to say something was true because it matched reality.<br>Now, we say something is credible because we understand <em>how</em> it was created, <em>why</em>, and <em>by whom</em>.<br>That kind of certainty is more fragile â€” but perhaps more honest.â€</p>
</blockquote>
<p>She nodded, not entirely convinced, and walked out.<br>I was left alone in the hallway. The boats at the dock were glowing orange in the dusk.</p>
<p>AI is redefining knowledge, truth, and reality.<br>We can participate in that process â€” not as passive users,<br>but as empowered citizens, as co-creators of meaning.  </p>
<p>What we need is not a better truth-detector,<br>but the <strong>ability to swim in a sea of shifting truths</strong>.  </p>
<p>Not to fight the current,<br>but to understand its direction, know our position within it,<br>and choose where we want to go.</p>
<p>It wonâ€™t be easy.<br>But this is the survival skill we must now learn.</p>
<hr>
<h2 id="Epilogue"><a href="#Epilogue" class="headerlink" title="Epilogue"></a>Epilogue</h2><p>The streetlights are on.<br>I pack up my things and step out of the library.<br>The air after the rain smells clean.<br>People walk past me, heads down, their screens glowing.  </p>
<p>I take out my own phone, open a news app.<br>The algorithm has already curated todayâ€™s world for me.</p>
<p>I donâ€™t long for 2019 as much anymore.</p>
<p>And maybe, I donâ€™t hate the rain as much either.</p>
<hr>
<h2 id="A-Note-on-This-Essay"><a href="#A-Note-on-This-Essay" class="headerlink" title="A Note on This Essay"></a>A Note on This Essay</h2><p>This piece originated from a final paper I wrote during the summer term.<br>Iâ€™m grateful to my professor for offering such thought-provoking prompts.<br>(Though yes, the academic pressure was real ğŸ˜….)</p>
<p>Six months later, I revisited the topic with fresh thoughts and wanted to share a revised version here.<br>Thank you for reading.</p>
<p>If youâ€™ve made it this far, you might be wondering: <em>What can I do now?</em></p>
<p>Here are a few suggestions:</p>
<ul>
<li>When using ChatGPT, pay attention to how its responses shape your thinking.</li>
<li>When you see a piece of news, ask not just <em>â€œIs this true?â€</em> but also <em>â€œWho is saying this, and why now?â€</em></li>
<li>Check your app settings â€” see how your feed is being personalized.</li>
<li>Pick up an introductory book on AI.</li>
<li>Follow your countryâ€™s legislative discussions on AI regulation.</li>
<li>Talk to your friends and family about how they use AI.<br>(Or drop me a message â€” Iâ€™d love to hear from you.)</li>
</ul>
<p>If youâ€™re an educator, consider how to integrate â€œunderstanding AIâ€ into your curriculum.<br>If youâ€™re a policymaker, push for stronger platform accountability.<br>If youâ€™re just a regular person, like me â€” remember:<br><strong>You have the right to demand a more transparent, more just digital world.</strong></p>
<p>All change begins with a question.</p>
<hr>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li>Hayles, N. Katherine (2025). <em>Bacteria to AI: Human Futures with our Nonhuman Symbionts</em>. University of Chicago Press.  </li>
<li>Pasquinelli, Matteo (2023). <em>The Eye of the Master: A Social History of Artificial Intelligence</em>. Verso Books.  </li>
<li>Rouvroy, Antoinette (2022). â€œAlgorithmic Governmentality,â€ in <em>More Posthuman Glossary</em>. Bloomsbury Academic.  </li>
<li>UNESCO (2023). <em>Guidelines for the Governance of Digital Platforms</em>.</li>
</ul>
</div><div class="post-end"><div class="post-prev"><a href="/2025/11/02/svanarna-dansa/" title="Previous Article"><i class="fa-solid fa-chevron-left fa-lg"></i></a></div><div class="post-next"><a href="/2025/10/13/jag-glommer-inte/" title="Next Article"><i class="fa-solid fa-chevron-right fa-lg"></i></a></div></div></article><div id="post-toc"><aside class="toc-aside"><div class="toc-title"><span><i class="fa-solid fa-paw"></i>TOC</span></div><div class="toc-container" id="toc-body"><ol class="toc-content"><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#I-The-Pope-in-a-Puffer-Jacket"><span class="toc-content-number">1.</span> <span class="toc-content-text">I. The Pope in a Puffer Jacket</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#II-The-Illusion-of-Stability"><span class="toc-content-number">2.</span> <span class="toc-content-text">II. The Illusion of Stability</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#III-After-the-Enlightenment"><span class="toc-content-number">3.</span> <span class="toc-content-text">III. After the Enlightenment</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#IV-What-Do-We-Need"><span class="toc-content-number">4.</span> <span class="toc-content-text">IV. What Do We Need?</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#V-Living-in-the-Flow"><span class="toc-content-number">5.</span> <span class="toc-content-text">V. Living in the Flow</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#Epilogue"><span class="toc-content-number">6.</span> <span class="toc-content-text">Epilogue</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#A-Note-on-This-Essay"><span class="toc-content-number">7.</span> <span class="toc-content-text">A Note on This Essay</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#References"><span class="toc-content-number">8.</span> <span class="toc-content-text">References</span></a></li></ol></div></aside><div class="toc-blank" onclick="tocToggle()"></div></div></div></div><div id="tool-bar"><div id="tool-bar-main"><div id="tool-toggle" onclick="toolToggle()" title="Settings"><i class="fa-solid fa-gear"></i></div><div id="toc-toggle" onclick="tocToggle()" title="TOC"><i class="fa-solid fa-list-ul"></i></div><div id="back-to-top" onclick="scrollToTop()" title="Back To Top"><i class="fa-solid fa-chevron-up"></i></div></div><div id="tool-bar-more" style="display: none;"><div id="darkmode-switch" onclick="darkmodeSwitch()" title="Darkmode"><i class="fa-solid fa-circle-half-stroke"></i></div><div id="font-size-increase" onclick="fontSizeIncrease()" title="Increase Font Size"><i class="fa-solid fa-plus"></i></div><div id="font-size-decrease" onclick="fontSizeDecrease()" title="Decrease Font Size"><i class="fa-solid fa-minus"></i></div></div></div><div id="search-panel"><div class="search-container"><div class="search-head"><div class="search-title"><span><i class="fa-solid fa-paw"></i>Search</span></div><div class="search-close-btn" onclick="toggleSearchWindow()"><i class="fa-regular fa-circle-xmark"></i></div></div><div class="search-box"><i class="fa-solid fa-magnifying-glass"></i><input id="search-input" type="text" placeholder="Please enter the content to search..." value=""/></div><div class="search-body"><div id="search-count">Matching results: </div><div id="search-result"></div><div id="search-result-empty">No matching articles were found.</div></div></div></div><footer><div class="footer-content"><div class="copyright-info"><i class="fa-regular fa-copyright fa-xs"></i><span>2017 - 2025 </span><a href="/about">Luna Tian</a><i class="fa-solid fa-cat fa-sm"></i><span>Powered by </span><a href="https://hexo.io/" target="_blank">Hexo</a><span> &amp; </span><a href="https://github.com/chanwj/hexo-theme-meow" target="_blank" title="v2.1.5">Theme Meow</a></div></div></footer>
<script>const GLOBAL_CONFIG = {
  comment: { theme: ''}
}
</script>
<script src="/js/third-party/darkmode.js"></script>
<script>var options = {
  dark: '/css/darkmode.css',
  startAt: '20:00',
  endAt: '06:00',
  checkSystemScheme: 'false',
  saveOnToggle: 'true'
};
var darkMode = new DarkMode(options);
// change comment theme synchronously åŒæ­¥ä¿®æ”¹è¯„è®ºåŒºä¸»é¢˜
if (darkMode.getMode() == "dark" && (false || false)) {
  if (document.getElementById('comment')) {
    document.getElementById('comment').getElementsByTagName('script')[0].setAttribute('data-theme', 'noborder_dark');
  }
}
</script><script>if (localStorage.getItem('font-size')) {
  document.querySelector('.post-content').style.fontSize = localStorage.getItem('font-size') + 'px';
}
</script>
<script src="/js/theme/tool-bar.js"></script>


<script src="/js/theme/menu.js"></script>


<script src="/js/third-party/clipboard.min.js"></script>


<script src="/js/theme/copy.js"></script>
<script>copyCode();
</script></body></html>