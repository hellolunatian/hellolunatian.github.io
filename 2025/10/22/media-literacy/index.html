<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="author" content="Luna Tian"/><meta name="keyword"/><meta name="description" content="Luna Tian Today, the question we face may not be “What can we still trust?” but rather, “How can we live responsibly in an age where truth itself is unstable?”   AI, like every other transformative te">
<meta property="og:type" content="article">
<meta property="og:title" content="Can Media Literacy Save Us from the Illusions of AI?">
<meta property="og:url" content="https://lunasblog.com/2025/10/22/media-literacy/index.html">
<meta property="og:site_name" content="Luna Tian">
<meta property="og:description" content="Luna Tian Today, the question we face may not be “What can we still trust?” but rather, “How can we live responsibly in an age where truth itself is unstable?”   AI, like every other transformative te">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://lunasblog.com/img/pope.png">
<meta property="article:published_time" content="2025-10-22T18:17:07.000Z">
<meta property="article:modified_time" content="2025-10-22T18:22:08.099Z">
<meta property="article:author" content="Luna Tian">
<meta property="article:tag" content="AI and Journalism">
<meta property="article:tag" content="Media Literacy">
<meta property="article:tag" content="Education">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lunasblog.com/img/pope.png"><title>Can Media Literacy Save Us from the Illusions of AI? - Luna Tian - Journalist & Blogger</title><link rel="shortcut icon" href="/img/site-icon.png">
<link rel="stylesheet" href="/css/style.css" id="dm-light">


<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css">
<meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="top-nav" ondblclick="scrollToTop()"><div class="nav-info"><div class="nav-icon"><img id="nav-icon" src="/img/site-icon.png"/></div><div class="nav-title"><a id="nav-title" href="/" title="Home">Luna Tian</a></div></div><div class="nav-ribbon"><div class="top-menu-expanded"><a class="top-menu-item" href="/archives"><span>Archives</span></a><a class="top-menu-item" href="/categories"><span>Categories</span></a><a class="top-menu-item" href="/tags"><span>Tags</span></a><a class="top-menu-item" href="/about"><span>About</span></a></div><div id="top-menu-btn" onclick="openTopMenu()" title="Open Menu"><i class="fa-solid fa-bars fa-lg"></i></div></div></div></header><div id="top-menu-hidden"><div class="menu-hidden-content"><div class="menu-hidden-nav"><a class="menu-hidden-item" href="/archives"><i class="fa-solid fa-box-archive fa-sm"></i><span>Archives</span></a><a class="menu-hidden-item" href="/categories"><i class="fa-regular fa-folder-open fa-sm"></i><span>Categories</span></a><a class="menu-hidden-item" href="/tags"><i class="fa-solid fa-tags fa-sm"></i><span>Tags</span></a><a class="menu-hidden-item" href="/about"><i class="fa-solid fa-paw fa-sm"></i><span>About</span></a></div></div><div class="menu-hidden-blank" onclick="closeTopMenu()"></div></div>
<div class="blog-info"><div class="blog-pic"><img id="blog-pic" src="/img/site-icon.png"/></div><div class="blog-title"><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i><span>Luna Tian</span><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i></div><div class="blog-desc">Tracking freedom, truth, and memory — one story at a time.</div></div><div class="main"><div class="main-content"><article class="post"><div class="post-title"><h1><i class="fa-solid fa-paw"></i>Can Media Literacy Save Us from the Illusions of AI?</h1></div><div class="post-info"><div class="post-info-first-line"><div class="post-date"><i class="icon fa-regular fa-calendar-plus" title="Published date"></i><time class="publish-time">2025-10-22</time><!-- i.icon.fa-regular.fa-calendar-check(title= __('datetime.updated'))--><!-- time.update-time= date(post.updated)--></div>
<div class="post-categories"><i class="icon fa-regular fa-folder-open" title="Categories"></i><a class="post-category" href="/categories/Research/">Research</a></div>
<div class="post-tags"><i class="icon fa-solid fa-tags" title="Tags"></i><a class="post-tag" href="/tags/AI-and-Journalism/">AI and Journalism</a><i class="icon fa-solid fa-tags" title="Tags"></i><a class="post-tag" href="/tags/Media-Literacy/">Media Literacy</a><i class="icon fa-solid fa-tags" title="Tags"></i><a class="post-tag" href="/tags/Education/">Education</a></div></div><div class="post-info-second-line"><div class="post-copyright"><i class="icon fa-brands fa-creative-commons" title="Copyright"></i><span>Copyright: </span><a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" title="CC BY-NC-ND 4.0">BY-NC-ND 4.0</a></div>
<div class="post-word-count"><i class="icon fa-solid fa-pen-to-square"></i><span>About 3357 words</span></div></div></div><div class="post-content"><p>Luna Tian</p>
<p>Today, the question we face may not be <em>“What can we still trust?”</em> but rather, <em>“How can we live responsibly in an age where truth itself is unstable?”</em>  </p>
<p>AI, like every other transformative technology, has no reverse gear. Once it starts, it doesn’t stop. The line between the real and the artificial will only continue to blur as AI-generated content multiplies.  </p>
<p>Perhaps media literacy programs, which we keep reading about, embody a belief — that education can help people reclaim control over truth.<br>But maybe the point is not about <strong>control</strong>, but about <strong>coexistence</strong>.</p>
<hr>
<h2 id="I-The-Pope-in-a-Puffer-Jacket"><a href="#I-The-Pope-in-a-Puffer-Jacket" class="headerlink" title="I. The Pope in a Puffer Jacket"></a>I. The Pope in a Puffer Jacket</h2><p>I still remember one of the most striking pieces of news from March 2023:<br>Pope Francis appeared online wearing a white Balenciaga puffer coat.  </p>
<p align="center">
  <img src="/img/pope.png" width="80%">
  <br>
  <em>Fake photos of Pope Francis in a puffer jacket.</em>
</p>

<p>The photo made me laugh. There he was, calm and dignified, walking down the street in that voluminous, glossy jacket — a sharp contrast to his usual modest robes. The coat was beautiful, its fabric full and rich in texture.  </p>
<p>Within hours, the image went viral, shared millions of times. Some people laughed like I did; some discussed the Catholic Church’s public image strategy; others admired the outfit, even asking me whether I’d consider buying one.<br>For a moment, I honestly thought it was part of a marketing campaign.  </p>
<p>Later, I learned that the image was created by a construction worker in Chicago. He had simply typed a few words into Midjourney — “Pope,” “Balenciaga,” “street photo.” Seconds later, the algorithm produced it: the lighting perfect, the texture of the coat realistic, even the Pope’s slightly hunched posture eerily convincing.</p>
<p>Three days later, fact-checking organizations confirmed that it was AI-generated.<br>But by then, the image had already been reprinted by thousands of media outlets, discussed endlessly online. In some sense, it had already <em>become an event.</em></p>
<p>I noticed one user on Twitter who asked:  </p>
<blockquote>
<p>“If even this is fake, what can we still believe in?”</p>
</blockquote>
<p>At the time, I didn’t have an answer.<br>Eight months later, UNESCO published the <em>Guidelines for the Governance of Digital Platforms</em> — a document that tried to impose order on our chaotic digital landscape.  </p>
<p>Its core recommendations could be summarized as three principles:  </p>
<ol>
<li>Foster critical thinking  </li>
<li>Promote fact-checking  </li>
<li>Urge platforms to label AI-generated content</li>
</ol>
<p>Learn to distinguish the true from the false — and you can remain clear-minded amid AI’s illusions. That was its logic.</p>
<p>The first time I read this document was in May last year, while finishing a term paper. It was nearly summer; everything felt heavy; I was in a bad mood — and, as luck would have it, it was raining.  </p>
<p>Still, I found the document well written. I’ve always had a soft spot for texts filled with goodwill and idealism.<br>But I stopped at page nineteen.  </p>
<p>There, it said: platforms should be <em>urged</em> to label AI-generated content. Not <em>required</em>, not <em>mandated</em>, but <em>urged</em> — a word so polite it’s nearly transparent.  </p>
<p>That night, out of boredom, I opened ChatGPT and asked it to generate a short paragraph about media literacy, citing three academic papers.  </p>
<p>It responded instantly: eloquent prose, coherent structure, three papers listed with author names, titles, and journal names.  </p>
<p>When I checked the references, none of them existed.</p>
<p>It wasn’t the first time, and yet I couldn’t help laughing. The authors’ names were real; the journals were real; the paper titles sounded perfectly plausible — but the works themselves had never been written.  </p>
<p>I asked the AI to describe these “papers.”<br>Its made-up arguments sometimes sounded more persuasive than real scholarship — as if they existed somewhere in a parallel universe, waiting to be cited. The thought sent a shiver through me.</p>
<p>So, is our biggest challenge still <em>“how to tell true from false”</em>?<br>I think not. The real question is: <strong>what counts as truth?</strong></p>
<hr>
<h2 id="II-The-Illusion-of-Stability"><a href="#II-The-Illusion-of-Stability" class="headerlink" title="II. The Illusion of Stability"></a>II. The Illusion of Stability</h2><p>If we look back — and “back” may mean only two years ago now — UNESCO’s guidelines were born in a peculiar moment of history.  </p>
<p>Let’s return to 2023: ChatGPT had just entered the public eye; Midjourney images were flooding social media; governments worldwide were scrambling to respond.<br>UNESCO’s chosen path was the familiar one: education.</p>
<blockquote>
<p>“Empowering users — including through media and information literacy education — is fundamental to countering disinformation and promoting healthy online participation,”<br>the document reads on page eleven.</p>
</blockquote>
<p>It’s a familiar logic.<br>It assumes the problem lies with the user — not as a form of blame, but as a premise: if people can be made smarter and more critical, then they can resist misinformation.  </p>
<p>Learn critical thinking, practice fact-checking, and you’ll survive the data flood.  </p>
<p>But the language of the document is telling. Words like “should,” “encourage,” and “promote” abound, while “must” and “require” are scarce.  </p>
<p>When it comes to platform responsibility, the tone becomes almost evasive.<br>In discussing algorithmic transparency, the text suggests that platforms <em>“should consider”</em> disclosing information about their recommendation systems.</p>
<p><em>Should consider</em> — meaning, of course, they could also <em>not</em> consider it.</p>
<p>I closed my laptop and went for a walk, still thinking about what the document <em>didn’t</em> say.<br>What happens when there are no penalties, no oversight bodies, no accountability?<br>What if platforms simply choose not to label AI content?<br>If algorithms remain opaque, who is responsible?</p>
<p>Unfortunately, the document is silent on that.</p>
<p>I recalled a Zoom conference I once attended. A policymaker said on stage:  </p>
<blockquote>
<p>“We must empower every citizen to distinguish truth from falsehood.”  </p>
</blockquote>
<p>Thunderous applause followed.  </p>
<p>During the Q&amp;A, a woman introduced herself as a high school teacher and asked:  </p>
<blockquote>
<p>“If my students barely have time to finish their homework, how much time should they spend verifying the news every day?”  </p>
</blockquote>
<p>Moments later, I logged out.<br>The panelists praised her “excellent question,” promised to “add more class hours,” and moved on.<br>I stood by the library window. It was already getting dark.  </p>
<p>Let’s imagine, then, a thought experiment.</p>
<p>Suppose you are a responsible citizen.<br>You follow every UNESCO recommendation.</p>
<p>One day, you read a piece of news: a certain government is secretly developing a new weapon.<br>You check the author’s profile — it looks legitimate.<br>You search for corroboration — several small media outlets have reported the same.<br>You find documents that look official.<br>You cross-check multiple sources. Everything aligns.  </p>
<p>So, you share it.</p>
<p>Three days later, a fact-checking organization announces that it was a coordinated disinformation campaign.<br>Those “small media outlets” were fabricated. The “official documents” were AI-generated.<br>The “ordinary user” was a bot.<br>The entire event was an AI-driven, multilayered illusion.</p>
<p>As I once wrote in another essay, <em>“Things That Look Like News,”</em> — a text that <em>looks</em> like journalism feels trustworthy. Likewise, something that <em>looks</em> like truth can bypass our verification instincts.</p>
<p>You did everything right. You followed every guideline.<br>And still, you were fooled.</p>
<p>Because the rules of the game have changed.</p>
<hr>
<h2 id="III-After-the-Enlightenment"><a href="#III-After-the-Enlightenment" class="headerlink" title="III. After the Enlightenment"></a>III. After the Enlightenment</h2><p>The French philosopher Antoinette Rouvroy once introduced a concept called <strong>“algorithmic governmentality”</strong> during a talk at the <em>Society of the Query</em> conference.  </p>
<p>She explained it simply:<br>In the digital age, power no longer operates by issuing orders; it operates by shaping environments.<br>Platforms don’t have to tell you what to see — they just adjust the algorithms, and you’ll naturally see what they want you to see.</p>
<p>So, does media literacy still matter?</p>
<p>Maybe.<br>But if it only teaches you how to find your way out of a maze without ever asking <em>why the maze exists</em>, it risks becoming another tool of governance.<br>Education should teach us to question and to critique.</p>
<p>What does an ideal human being look like?<br>Perhaps rational, autonomous, capable of independent judgment — the Enlightenment model of <em>Descartes’ “I think, therefore I am”</em> and <em>Kant’s “Dare to know.”</em>  </p>
<p>In that tradition, humans are subjects; technology is a tool. The boundary between them is clear.  </p>
<p>But does that boundary still exist?</p>
<hr>
<p>These days, I have new habits that I never used to have.<br>When I hit a wall in writing, I used to email a trusted friend or text a kind professor. We’d discuss ideas, and I’d often come away inspired.<br>It slowed my writing, but those conversations were warm and human. I remember the office hours that turned into dinners at favorite restaurants. I’ve never been a solitary writer; I like the feeling of thinking <em>with</em> others.</p>
<p>ChatGPT has changed that.<br>Although I avoid using AI in my essays or academic work, I still appreciate its instant feedback — the kind that comes without judgment or social weight.<br>Sometimes I ask it questions, seek suggestions, explore threads of thought.  </p>
<p>But mid-conversation, I often pause and wonder: <em>Who is writing this?</em><br>Is it me, or the AI?<br>Or have we, somehow, co-authored something?</p>
<p>I recall reading the literary theorist N. Katherine Hayles, who developed the concept of the <strong>“posthuman.”</strong><br>She observed that our thinking, memory, and judgment are now so deeply embedded in technical systems that it’s nearly impossible to say where the human ends and the machine begins.</p>
<p>As someone who grew up reading science fiction, I once found that idea exciting.<br>Now that I’m living it, I’m not sure whether to call it a privilege or a tragedy.</p>
<p>Hayles wrote:  </p>
<blockquote>
<p>“The question is not how to resist AI’s influence, but how to remain ethical, reflective, and responsible within a state of coexistence.”  </p>
</blockquote>
<p>Traditional media literacy teaches us to be better <em>independent judges</em>.<br>But what if judgment itself is no longer an independent act?<br>If AI not only transmits information but shapes how we perceive the world — does “independent judgment” still mean anything?</p>
<p>A few days ago, I called a classmate who studies the philosophy of AI.<br>I told him about my anxiety — about how overwhelming this all feels — and asked what troubled him most about large language models.  </p>
<p>He thought for a while and said:  </p>
<blockquote>
<p>“They’re not reproducing human knowledge. They’re generating a new kind of knowledge.”  </p>
</blockquote>
<p>“What do you mean?” I asked.  </p>
<p>“Traditionally,” he said, “knowledge comes from evidence, experience, reasoning. A statement is true because it corresponds to reality.<br>But GPT doesn’t work that way. It calculates statistical relationships between words. It predicts which sentences sound plausible.<br>The result is something neither true nor false — just <em>apparently believable</em>.”  </p>
<p>I told him about the fake citations ChatGPT had produced for me.<br>He said, “That’s not deception in the traditional sense. It’s a new form of production. It’s not giving you lies — it’s giving you <em>possibilities</em>.”</p>
<h2 id="IV-What-Do-We-Need"><a href="#IV-What-Do-We-Need" class="headerlink" title="IV. What Do We Need?"></a>IV. What Do We Need?</h2><p>There’s no simple answer.<br>But perhaps we can begin by reframing the question.</p>
<p>Not “Is this true or false?” — but  </p>
<blockquote>
<p>“How was this created?”<br>Not “How can I identify AI content?” — but<br>“How is AI shaping the way I think?”<br>Not “How do I protect myself?” — but<br>“How do we collectively build a more transparent environment?”</p>
</blockquote>
<p>This requires a new kind of awareness.</p>
<p>First, a reflective mindset.<br>It may be hard to reject AI entirely in daily life, but staying conscious while using it is still possible.<br>When you ask ChatGPT to help write an email, when you accept algorithmic news recommendations, when you feel convinced by a well-written post — pause. Step back. Ask yourself:  </p>
<blockquote>
<p>Why does this feel trustworthy?<br>Because it makes sense? Or because it aligns with what I already want to believe?</p>
</blockquote>
<p>It’s not about one-time fact-checking. It’s about cultivating a state of ongoing dialogue — a kind of <strong>awareness</strong>.  </p>
<p>Even if you’re co-creating meaning with AI, you can still remain conscious of that process.</p>
<p>Then, there’s systems thinking.<br>Information doesn’t appear out of nowhere. It is produced, filtered, and circulated within complex technological, economic, and political systems.<br>When you read a headline that begins with <em>“Studies show…”</em>, don’t just ask whether the study exists — ask:  </p>
<blockquote>
<p>Who funded this study?<br>Where did the data come from?<br>Why is this narrative being amplified <em>now</em>?</p>
</blockquote>
<p>Some technical literacy is also helpful.<br>You don’t need to learn how to code, but understanding the basics helps:  </p>
<ul>
<li>How do recommendation algorithms determine what we see?  </li>
<li>Why do language models hallucinate?  </li>
<li>How does training data shape AI bias?</li>
</ul>
<p>We don’t need technical mastery.<br>We need <strong>power literacy</strong> — to understand <em>who</em> shapes our information environment, and <em>how</em>.</p>
<p>But most important of all: we must shift from <strong>individual responsibility</strong> to <strong>collective action</strong>.</p>
<p>No matter how smart you are as an individual, you can’t fight systemic manipulation alone.<br>You may learn to detect fake news. But if an algorithm is pushing carefully designed disinformation to a billion users every day — your personal resistance won’t be enough.</p>
<p>We need a concept of <strong>digital citizenship</strong>.<br>The right to know what algorithms are doing to you.<br>The right to control how your data is used.<br>The right to demand explanations when AI systems make decisions that affect your life.<br>The right to opt out of personalized recommendations.  </p>
<p>These aren’t gifts from tech companies.<br>They are <strong>rights</strong> — to be won through legislation, regulation, and collective negotiation.</p>
<p>The EU is already experimenting.<br>The <strong>AI Act</strong> passed in 2024 mandates that high-risk AI systems must be explainable and human-supervised.<br>The <strong>Digital Services Act</strong> gives users the right to opt out of personalization, and forces major platforms to explain their algorithmic logic to regulators.</p>
<p>These laws are imperfect, their enforcement full of challenges.<br>But they acknowledge one essential truth: <strong>we cannot place all responsibility on the user</strong>.</p>
<p>Some say the EU is too conservative.<br>But I think there’s value in trying to pull the reins.</p>
<p>Of course, legislation always lags behind.<br>By the time we’re regulating GPT-4, GPT-5 is already here.<br>But that’s not a reason to give up — it’s a reason to act faster, and smarter.</p>
<p>The hardest question to ignore is: <strong>what responsibility do the platforms bear?</strong></p>
<p>Why are we always talking about “educating users,”<br>but almost never about <strong>regulating platforms</strong>?</p>
<p>If a food company sells toxic food, we don’t say, <em>“Consumers should learn to detect poison.”</em><br>But in the digital world, we’ve accepted a strange logic: platforms can design systems that spread lies — then blame the user for believing them.</p>
<p>That logic must be challenged.<br>Algorithms decide what gets seen, amplified, and recommended.<br>If that’s editorial power, then <strong>editorial responsibility must follow</strong>.</p>
<p>Concretely:  </p>
<ul>
<li>Platforms must make recommendation algorithms transparent to regulators.  </li>
<li>If disinformation causes real-world harm, platforms should be held legally accountable.  </li>
<li>Users should have the right to opt out of personalization.</li>
</ul>
<p>As for <em>“AI-generated content must be labeled”</em> — honestly, that may be the most difficult goal of all.</p>
<p>There are enormous technical challenges.<br>Who decides what counts as “AI-generated”?<br>What if a text is 50% AI-written, 50% human-edited — does it count?<br>And what about enforcement? Anonymous uploads? Cross-border platforms?</p>
<p>Maybe perfection isn’t the goal.<br>Maybe the goal is <strong>procedural justice</strong>.</p>
<p>In high-risk domains — journalism, medicine, legal documents — disclosure of AI use should be mandatory.<br>Platforms can offer creators labeling tools.<br>We can invest in detection technologies — though it may become an endless arms race.</p>
<p>The point is not to design a perfect system,<br>but to <strong>stop offloading all the burden onto individuals</strong>.</p>
<hr>
<h2 id="V-Living-in-the-Flow"><a href="#V-Living-in-the-Flow" class="headerlink" title="V. Living in the Flow"></a>V. Living in the Flow</h2><p>As I write this, it’s not raining — but it is windy.<br>The temperature dropped nearly five degrees today.<br>I might need my down jacket tomorrow.</p>
<p>Speaking of jackets…<br>That image of the Pope in the puffer coat comes back to me.<br>It looked so real. I even thought it was brand marketing. But it wasn’t. It was misinformation.  </p>
<p>And when people found out, they didn’t get angry.<br>They got confused.</p>
<blockquote>
<p>“If even this is fake, what can we still believe in?”</p>
</blockquote>
<p>Maybe that question itself is flawed.</p>
<p>The point isn’t <em>what</em> we can still believe in —<br>but <em>how</em> we live <strong>ethically in an age where truth is fluid</strong>.</p>
<p>Like all technology, AI can’t be un-invented.<br>We’re heading into a future where content will be increasingly generated,<br>and the line between fact and fiction will continue to dissolve.  </p>
<p>We cannot return to a time when truth was clear —<br>and perhaps that time never existed.</p>
<p>Even before AI, we were already surrounded by misinformation.</p>
<p>Still, I believe UNESCO’s media literacy framework represents a sincere conviction —<br>a belief that education can help us regain control over truth.</p>
<p>But <strong>control</strong> is not the answer.<br><strong>Coexistence</strong> is.</p>
<p>That doesn’t mean abandoning judgment.<br>It means cultivating a <strong>more complex kind of literacy</strong> —<br>one that reflects while using AI, questions the logic of production,<br>and seeks <strong>collective change</strong> beyond personal efforts.</p>
<p>It means recognizing that while we can no longer stand <em>outside</em> technology,<br>we can still remain <strong>awake within it</strong>.</p>
<p>Last fall, while giving a presentation, a student sighed and said,  </p>
<blockquote>
<p>“It sounds like we can’t be sure of anything anymore.”</p>
</blockquote>
<p>I thought for a moment and replied:  </p>
<blockquote>
<p>“It’s not that nothing can be certain. It’s that the <em>way</em> we become certain has changed.<br>We used to say something was true because it matched reality.<br>Now, we say something is credible because we understand <em>how</em> it was created, <em>why</em>, and <em>by whom</em>.<br>That kind of certainty is more fragile — but perhaps more honest.”</p>
</blockquote>
<p>She nodded, not entirely convinced, and walked out.<br>I was left alone in the hallway. The boats at the dock were glowing orange in the dusk.</p>
<p>AI is redefining knowledge, truth, and reality.<br>We can participate in that process — not as passive users,<br>but as empowered citizens, as co-creators of meaning.  </p>
<p>What we need is not a better truth-detector,<br>but the <strong>ability to swim in a sea of shifting truths</strong>.  </p>
<p>Not to fight the current,<br>but to understand its direction, know our position within it,<br>and choose where we want to go.</p>
<p>It won’t be easy.<br>But this is the survival skill we must now learn.</p>
<hr>
<h2 id="Epilogue"><a href="#Epilogue" class="headerlink" title="Epilogue"></a>Epilogue</h2><p>The streetlights are on.<br>I pack up my things and step out of the library.<br>The air after the rain smells clean.<br>People walk past me, heads down, their screens glowing.  </p>
<p>I take out my own phone, open a news app.<br>The algorithm has already curated today’s world for me.</p>
<p>I don’t long for 2019 as much anymore.</p>
<p>And maybe, I don’t hate the rain as much either.</p>
<hr>
<h2 id="A-Note-on-This-Essay"><a href="#A-Note-on-This-Essay" class="headerlink" title="A Note on This Essay"></a>A Note on This Essay</h2><p>This piece originated from a final paper I wrote during the summer term.<br>I’m grateful to my professor for offering such thought-provoking prompts.<br>(Though yes, the academic pressure was real 😅.)</p>
<p>Six months later, I revisited the topic with fresh thoughts and wanted to share a revised version here.<br>Thank you for reading.</p>
<p>If you’ve made it this far, you might be wondering: <em>What can I do now?</em></p>
<p>Here are a few suggestions:</p>
<ul>
<li>When using ChatGPT, pay attention to how its responses shape your thinking.</li>
<li>When you see a piece of news, ask not just <em>“Is this true?”</em> but also <em>“Who is saying this, and why now?”</em></li>
<li>Check your app settings — see how your feed is being personalized.</li>
<li>Pick up an introductory book on AI.</li>
<li>Follow your country’s legislative discussions on AI regulation.</li>
<li>Talk to your friends and family about how they use AI.<br>(Or drop me a message — I’d love to hear from you.)</li>
</ul>
<p>If you’re an educator, consider how to integrate “understanding AI” into your curriculum.<br>If you’re a policymaker, push for stronger platform accountability.<br>If you’re just a regular person, like me — remember:<br><strong>You have the right to demand a more transparent, more just digital world.</strong></p>
<p>All change begins with a question.</p>
<hr>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li>Hayles, N. Katherine (2025). <em>Bacteria to AI: Human Futures with our Nonhuman Symbionts</em>. University of Chicago Press.  </li>
<li>Pasquinelli, Matteo (2023). <em>The Eye of the Master: A Social History of Artificial Intelligence</em>. Verso Books.  </li>
<li>Rouvroy, Antoinette (2022). “Algorithmic Governmentality,” in <em>More Posthuman Glossary</em>. Bloomsbury Academic.  </li>
<li>UNESCO (2023). <em>Guidelines for the Governance of Digital Platforms</em>.</li>
</ul>
</div><div class="post-end"><div class="post-prev"></div><div class="post-next"><a href="/2025/10/13/jag-glommer-inte/" title="Next Article"><i class="fa-solid fa-chevron-right fa-lg"></i></a></div></div></article><div id="post-toc"><aside class="toc-aside"><div class="toc-title"><span><i class="fa-solid fa-paw"></i>TOC</span></div><div class="toc-container" id="toc-body"><ol class="toc-content"><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#I-The-Pope-in-a-Puffer-Jacket"><span class="toc-content-number">1.</span> <span class="toc-content-text">I. The Pope in a Puffer Jacket</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#II-The-Illusion-of-Stability"><span class="toc-content-number">2.</span> <span class="toc-content-text">II. The Illusion of Stability</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#III-After-the-Enlightenment"><span class="toc-content-number">3.</span> <span class="toc-content-text">III. After the Enlightenment</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#IV-What-Do-We-Need"><span class="toc-content-number">4.</span> <span class="toc-content-text">IV. What Do We Need?</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#V-Living-in-the-Flow"><span class="toc-content-number">5.</span> <span class="toc-content-text">V. Living in the Flow</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#Epilogue"><span class="toc-content-number">6.</span> <span class="toc-content-text">Epilogue</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#A-Note-on-This-Essay"><span class="toc-content-number">7.</span> <span class="toc-content-text">A Note on This Essay</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#References"><span class="toc-content-number">8.</span> <span class="toc-content-text">References</span></a></li></ol></div></aside><div class="toc-blank" onclick="tocToggle()"></div></div></div></div><div id="tool-bar"><div id="tool-bar-main"><div id="tool-toggle" onclick="toolToggle()" title="Settings"><i class="fa-solid fa-gear"></i></div><div id="toc-toggle" onclick="tocToggle()" title="TOC"><i class="fa-solid fa-list-ul"></i></div><div id="back-to-top" onclick="scrollToTop()" title="Back To Top"><i class="fa-solid fa-chevron-up"></i></div></div><div id="tool-bar-more" style="display: none;"><div id="darkmode-switch" onclick="darkmodeSwitch()" title="Darkmode"><i class="fa-solid fa-circle-half-stroke"></i></div><div id="font-size-increase" onclick="fontSizeIncrease()" title="Increase Font Size"><i class="fa-solid fa-plus"></i></div><div id="font-size-decrease" onclick="fontSizeDecrease()" title="Decrease Font Size"><i class="fa-solid fa-minus"></i></div></div></div><div id="search-panel"><div class="search-container"><div class="search-head"><div class="search-title"><span><i class="fa-solid fa-paw"></i>Search</span></div><div class="search-close-btn" onclick="toggleSearchWindow()"><i class="fa-regular fa-circle-xmark"></i></div></div><div class="search-box"><i class="fa-solid fa-magnifying-glass"></i><input id="search-input" type="text" placeholder="Please enter the content to search..." value=""/></div><div class="search-body"><div id="search-count">Matching results: </div><div id="search-result"></div><div id="search-result-empty">No matching articles were found.</div></div></div></div><footer><div class="footer-content"><div class="copyright-info"><i class="fa-regular fa-copyright fa-xs"></i><span>2017 - 2025 </span><a href="/about">Luna Tian</a><i class="fa-solid fa-cat fa-sm"></i><span>Powered by </span><a href="https://hexo.io/" target="_blank">Hexo</a><span> &amp; </span><a href="https://github.com/chanwj/hexo-theme-meow" target="_blank" title="v2.1.5">Theme Meow</a></div></div></footer>
<script>const GLOBAL_CONFIG = {
  comment: { theme: ''}
}
</script>
<script src="/js/third-party/darkmode.js"></script>
<script>var options = {
  dark: '/css/darkmode.css',
  startAt: '20:00',
  endAt: '06:00',
  checkSystemScheme: 'false',
  saveOnToggle: 'true'
};
var darkMode = new DarkMode(options);
// change comment theme synchronously 同步修改评论区主题
if (darkMode.getMode() == "dark" && (false || false)) {
  if (document.getElementById('comment')) {
    document.getElementById('comment').getElementsByTagName('script')[0].setAttribute('data-theme', 'noborder_dark');
  }
}
</script><script>if (localStorage.getItem('font-size')) {
  document.querySelector('.post-content').style.fontSize = localStorage.getItem('font-size') + 'px';
}
</script>
<script src="/js/theme/tool-bar.js"></script>


<script src="/js/theme/menu.js"></script>


<script src="/js/third-party/clipboard.min.js"></script>


<script src="/js/theme/copy.js"></script>
<script>copyCode();
</script></body></html>